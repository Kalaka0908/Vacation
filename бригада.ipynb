{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src = https://fotovmire.ru/wp-content/uploads/2019/05/16767/mashinist-i-stazhjor-v-kabine-lokomotiva.jpg alt=\"drawing\" style=\"width:400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Контроль за вниманием локомотивной бригады."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Постановка задачи\n",
    "Создание системы контроля бдительности локомотивной бригады при управлении подвижным составом на основе нейросетевой аналитики, фиксация отвлечений от управления на мобильный телефон, личные электронные устройства, прочих отвлечений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Проблематика\n",
    "Работникам локомотивных бригад, в виду высокой степени ответственности их профессии, запрещено при ведении электропоезда пользоваться мобильными телефонами или иными гаджетами. Несмотря на это, периодически корневой причиной нарушений требований безопасности движения становится отвлечение локомотивной бригады от управления подвижным составом на мобильный телефон, личные электронные устройства и другие гаджеты.\n",
    "В 2022 году на инфраструктуре ОАО «РЖД» было зафиксировано 1911 случаев нарушения безопасности движения. Из них число проездов запрещающего показания светофора составило 21 случай, число сходов подвижного состава 82 случая. Основной причиной данных видов нарушения безопасности движения является отвлечение от управления подвижным составом.\n",
    "Предложенное решение позволит адресно проводить профилактическую работу с работниками локомотивных бригад в целях предотвращения отвлечений на мобильные телефоны."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Решение\n",
    "Решение кейса представляет собой прототип системы, способной анализировать видеофайлы на предмет отвлечения локомотивной бригады от управления на посторонние предметы (основное использование телефона или любого другого гаджета) и формировать по результатам анализа отчёт.\n",
    "Отчёт должен содержать: количество выявленных нарушений, типы нарушений, время нарушения на временной шкале видеофайла.\n",
    "При этом система должна определять именно факт использования телефона, а не наличие его в кадре. Не является нарушение кратковременное использование телефона, к примеру, активация экрана до 3х секунд для ознакомления со временем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем нужные для нас библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import random\n",
    "import pathlib\n",
    "import itertools\n",
    "import collections\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import remotezip as rz\n",
    "\n",
    "import time\n",
    "from collections import Counter\n",
    "import logging\n",
    "import tkinter as tk\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import urllib.request\n",
    "import imageio\n",
    "from IPython import display\n",
    "from urllib import request\n",
    "from tensorflow_docs.vis import embed\n",
    "import sys\n",
    "import tarfile\n",
    "from urllib.request import urlretrieve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим и распакуем данные из архива, из внешнего источника (в данном случае из архива PASCAL VOC 2012, который является общедоступным набором данных для задач компьютерного зрения)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=-----------------------------------------------------------] 1.7%  \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====--------------------------------------------------------] 7.0%  \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===========-------------------------------------------------] 18.9%  \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========================------------------------------------] 40.0%  \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================--------------------------] 57.2%  \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================----------------------] 63.3%  \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=======================================---------------------] 65.1%  \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================================----------------] 73.3%  \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageSets====================================================] 100.0%  \n",
      "SegmentationClass\n",
      "SegmentationObject\n",
      "Annotations\n",
      "JPEGImages\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def progress_bar(count, total, status=\"\"):\n",
    "    bar_len = 60\n",
    "    filled_len = int(round(bar_len * count / float(total)))\n",
    "\n",
    "    percents = round(100.0 * count / float(total), 1)\n",
    "    bar = \"=\" * filled_len + \"-\" * (bar_len - filled_len)\n",
    "\n",
    "    sys.stdout.write(\"[{}] {}%  {}\\r\".format(bar, percents, status))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "def download_progress(block_num, block_size, total_size):\n",
    "    progress_bar(block_num * block_size, total_size)\n",
    "\n",
    "\n",
    "url = \"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\"\n",
    "output_directory = \"./data\"\n",
    "output_file = os.path.join(output_directory, \"voc2012_raw.tar\")\n",
    "extract_directory = os.path.join(output_directory, \"voc2012_raw\")\n",
    "\n",
    "# Создание папок, если они не существуют\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "os.makedirs(extract_directory, exist_ok=True)\n",
    "\n",
    "# Загрузка архива\n",
    "urlretrieve(url, output_file, reporthook=download_progress)\n",
    "\n",
    "# Извлечение содержимого архива\n",
    "with tarfile.open(output_file, \"r\") as tar:\n",
    "    tar.extractall(path=extract_directory)\n",
    "\n",
    "# Вывод содержимого извлеченной папки\n",
    "for item in os.listdir(os.path.join(extract_directory, \"VOCdevkit/VOC2012\")):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root = \"sf_data_science/Vacation\" \n",
    "data_root = os.path.join(root, \"pew_research_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Открываем и считываем данные из CSV-файла, указанного в file_path, и возвращаем эти данные в виде списка словарей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_file(file_path):\n",
    "    data = []\n",
    "    with open(file_path, encoding=\"utf-8\") as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            data.append(row)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполняем создание пути к файлу и его загрузку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(data_root, \"sf_data_science/Vacation/submission.csv\")\n",
    "file_data = load_file(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный код используется для предобработки и загрузки набора данных изображений для задачи сегментации из представленного в torchvision набора данных PASCAL VOC.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar to /Users/kakotichi/Documents/GitHub/sf_data_science/Vacation/VOCtrainval_11-May-2012.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1999639040/1999639040 [13:28<00:00, 2472487.87it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/kakotichi/Documents/GitHub/sf_data_science/Vacation/VOCtrainval_11-May-2012.tar to /Users/kakotichi/Documents/GitHub/sf_data_science/Vacation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "from torchvision.transforms import Compose, RandomHorizontalFlip\n",
    "\n",
    "# Указываем дополнительные настройки, если это необходимо\n",
    "seg_root = \"/sf_data_science/Vacation\"  # Замените '/some/path' на ваш путь к корневому каталогу данных\n",
    "image_set = \"train\"\n",
    "transformations = Compose([RandomHorizontalFlip(0.5)])\n",
    "\n",
    "# Создание набора данных для сегментации\n",
    "seg_dataset = VOCSegmentation(seg_root, year=\"2012\", image_set=image_set, transforms=transformations, download=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готовим для обучения модели компьютерного зрения в библиотеке torchvision, которая используется для обучения моделей глубокого обучения в PyTorch. Более конкретно, этот код используется для загрузки и предварительной обработки набора данных VOCSegmentation, который является популярным датасетом для задач семантической сегментации изображений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /Users/kakotichi/Documents/GitHub/sf_data_science/Vacation/VOCtrainval_11-May-2012.tar\n",
      "Extracting /Users/kakotichi/Documents/GitHub/sf_data_science/Vacation/VOCtrainval_11-May-2012.tar to /Users/kakotichi/Documents/GitHub/sf_data_science/Vacation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "from torchvision.transforms import Compose, RandomHorizontalFlip\n",
    "\n",
    "# Указываем дополнительные настройки, если это необходимо\n",
    "seg_root = \"/sf_data_science/Vacation\"  # Замените '/some/path' на ваш путь к корневому каталогу данных\n",
    "image_set = \"train\"\n",
    "transformations = Compose([RandomHorizontalFlip(0.5)])\n",
    "\n",
    "# Создание набора данных для сегментации\n",
    "seg_dataset = VOCSegmentation(seg_root, year=\"2012\", image_set=image_set, transforms=transformations, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем новый набор данных для обнаружения объектов, используя данные из набора данных Pascal VOC (Visual Object Classes) 2012. Это делается с помощью функции VOCDetection из библиотеки torchvision.datasets, которая загружает набор данных VOC.\n",
    "\n",
    "Набор данных Pascal VOC включает изображения с объектами различных классов. Он часто используется для задач компьютерного зрения, таких как распознавание объектов и сегментация изображений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /Users/kakotichi/Documents/GitHub/sf_data_science/Vacation/VOCtrainval_11-May-2012.tar\n",
      "Extracting /Users/kakotichi/Documents/GitHub/sf_data_science/Vacation/VOCtrainval_11-May-2012.tar to /Users/kakotichi/Documents/GitHub/sf_data_science/Vacation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torchvision.datasets import VOCDetection\n",
    "from torchvision.transforms import Compose, RandomHorizontalFlip\n",
    "\n",
    "# Указываем дополнительные настройки, если это необходимо\n",
    "det_root = \"/sf_data_science/Vacation\"  # Замените '/some/path' на ваш путь к корневому каталогу данных\n",
    "image_set = \"train\"\n",
    "transformations = Compose([RandomHorizontalFlip(0.5)])\n",
    "\n",
    "# Создание набора данных для обнаружения объектов\n",
    "det_dataset = VOCDetection(det_root, year=\"2012\", image_set=image_set, transforms=transformations, download=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем новый набор данных для глубокого обучения, объединяющий два разных набора данных из Pascal VOC: один набор данных для сегментации изображений (где каждому пикселю изображения присваивается семантическая метка класса), и другой для распознавания объектов (где каждому объекту на изображении присваивается ограничивающий прямоугольник и метка класса)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar to path/to/seg_data/VOCtrainval_11-May-2012.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1999639040/1999639040 [16:32<00:00, 2015403.74it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting path/to/seg_data/VOCtrainval_11-May-2012.tar to path/to/seg_data\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m transformations \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     29\u001b[0m download \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m combined_dataset \u001b[39m=\u001b[39m CombinedVOC(seg_root, det_root, year, image_set, transforms\u001b[39m=\u001b[39;49mtransformations, download\u001b[39m=\u001b[39;49mdownload)\n",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m, in \u001b[0;36mCombinedVOC.__init__\u001b[0;34m(self, seg_root, det_root, year, image_set, transforms, download)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, seg_root, det_root, year, image_set, transforms\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, download\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m----> 7\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseg_dataset \u001b[39m=\u001b[39m VOCSegmentation(seg_root, year\u001b[39m=\u001b[39;49myear, image_set\u001b[39m=\u001b[39;49mimage_set, transforms\u001b[39m=\u001b[39;49mtransforms, download\u001b[39m=\u001b[39;49mdownload)\n\u001b[1;32m      8\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdet_dataset \u001b[39m=\u001b[39m VOCDetection(det_root, year\u001b[39m=\u001b[39myear, image_set\u001b[39m=\u001b[39mimage_set, transforms\u001b[39m=\u001b[39mtransforms, download\u001b[39m=\u001b[39mdownload)\n\u001b[1;32m      9\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms \u001b[39m=\u001b[39m transforms\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/datasets/voc.py:98\u001b[0m, in \u001b[0;36m_VOCBase.__init__\u001b[0;34m(self, root, year, image_set, download, transform, target_transform, transforms)\u001b[0m\n\u001b[1;32m     95\u001b[0m voc_root \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot, base_dir)\n\u001b[1;32m     97\u001b[0m \u001b[39mif\u001b[39;00m download:\n\u001b[0;32m---> 98\u001b[0m     download_and_extract_archive(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murl, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot, filename\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilename, md5\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmd5)\n\u001b[1;32m    100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(voc_root):\n\u001b[1;32m    101\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/datasets/utils.py:438\u001b[0m, in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    436\u001b[0m archive \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(download_root, filename)\n\u001b[1;32m    437\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExtracting \u001b[39m\u001b[39m{\u001b[39;00marchive\u001b[39m}\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m{\u001b[39;00mextract_root\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 438\u001b[0m extract_archive(archive, extract_root, remove_finished)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/datasets/utils.py:413\u001b[0m, in \u001b[0;36mextract_archive\u001b[0;34m(from_path, to_path, remove_finished)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[39m# We don't need to check for a missing key here, since this was already done in _detect_file_type()\u001b[39;00m\n\u001b[1;32m    411\u001b[0m extractor \u001b[39m=\u001b[39m _ARCHIVE_EXTRACTORS[archive_type]\n\u001b[0;32m--> 413\u001b[0m extractor(from_path, to_path, compression)\n\u001b[1;32m    414\u001b[0m \u001b[39mif\u001b[39;00m remove_finished:\n\u001b[1;32m    415\u001b[0m     os\u001b[39m.\u001b[39mremove(from_path)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/datasets/utils.py:277\u001b[0m, in \u001b[0;36m_extract_tar\u001b[0;34m(from_path, to_path, compression)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_extract_tar\u001b[39m(from_path: \u001b[39mstr\u001b[39m, to_path: \u001b[39mstr\u001b[39m, compression: Optional[\u001b[39mstr\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    276\u001b[0m     \u001b[39mwith\u001b[39;00m tarfile\u001b[39m.\u001b[39mopen(from_path, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mr:\u001b[39m\u001b[39m{\u001b[39;00mcompression[\u001b[39m1\u001b[39m:]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m compression \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m tar:\n\u001b[0;32m--> 277\u001b[0m         tar\u001b[39m.\u001b[39;49mextractall(to_path)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/tarfile.py:2059\u001b[0m, in \u001b[0;36mTarFile.extractall\u001b[0;34m(self, path, members, numeric_owner)\u001b[0m\n\u001b[1;32m   2057\u001b[0m         tarinfo\u001b[39m.\u001b[39mmode \u001b[39m=\u001b[39m \u001b[39m0o700\u001b[39m\n\u001b[1;32m   2058\u001b[0m     \u001b[39m# Do not set_attrs directories, as we will do that further down\u001b[39;00m\n\u001b[0;32m-> 2059\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract(tarinfo, path, set_attrs\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m tarinfo\u001b[39m.\u001b[39;49misdir(),\n\u001b[1;32m   2060\u001b[0m                  numeric_owner\u001b[39m=\u001b[39;49mnumeric_owner)\n\u001b[1;32m   2062\u001b[0m \u001b[39m# Reverse sort directories.\u001b[39;00m\n\u001b[1;32m   2063\u001b[0m directories\u001b[39m.\u001b[39msort(key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m a: a\u001b[39m.\u001b[39mname)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/tarfile.py:2100\u001b[0m, in \u001b[0;36mTarFile.extract\u001b[0;34m(self, member, path, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2097\u001b[0m     tarinfo\u001b[39m.\u001b[39m_link_target \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path, tarinfo\u001b[39m.\u001b[39mlinkname)\n\u001b[1;32m   2099\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2100\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extract_member(tarinfo, os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(path, tarinfo\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m   2101\u001b[0m                          set_attrs\u001b[39m=\u001b[39;49mset_attrs,\n\u001b[1;32m   2102\u001b[0m                          numeric_owner\u001b[39m=\u001b[39;49mnumeric_owner)\n\u001b[1;32m   2103\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   2104\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrorlevel \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/tarfile.py:2173\u001b[0m, in \u001b[0;36mTarFile._extract_member\u001b[0;34m(self, tarinfo, targetpath, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2170\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dbg(\u001b[39m1\u001b[39m, tarinfo\u001b[39m.\u001b[39mname)\n\u001b[1;32m   2172\u001b[0m \u001b[39mif\u001b[39;00m tarinfo\u001b[39m.\u001b[39misreg():\n\u001b[0;32m-> 2173\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmakefile(tarinfo, targetpath)\n\u001b[1;32m   2174\u001b[0m \u001b[39melif\u001b[39;00m tarinfo\u001b[39m.\u001b[39misdir():\n\u001b[1;32m   2175\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmakedir(tarinfo, targetpath)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/tarfile.py:2222\u001b[0m, in \u001b[0;36mTarFile.makefile\u001b[0;34m(self, tarinfo, targetpath)\u001b[0m\n\u001b[1;32m   2220\u001b[0m     target\u001b[39m.\u001b[39mtruncate()\n\u001b[1;32m   2221\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2222\u001b[0m     copyfileobj(source, target, tarinfo\u001b[39m.\u001b[39;49msize, ReadError, bufsize)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/tarfile.py:251\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[0;34m(src, dst, length, exception, bufsize)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(buf) \u001b[39m<\u001b[39m bufsize:\n\u001b[1;32m    250\u001b[0m         \u001b[39mraise\u001b[39;00m exception(\u001b[39m\"\u001b[39m\u001b[39munexpected end of data\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 251\u001b[0m     dst\u001b[39m.\u001b[39mwrite(buf)\n\u001b[1;32m    253\u001b[0m \u001b[39mif\u001b[39;00m remainder \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    254\u001b[0m     buf \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39mread(remainder)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.datasets import VOCSegmentation, VOCDetection\n",
    "\n",
    "class CombinedVOC(Dataset):\n",
    "    def __init__(self, seg_root, det_root, year, image_set, transforms=None, download=True):\n",
    "        self.seg_dataset = VOCSegmentation(seg_root, year=year, image_set=image_set, transforms=transforms, download=download)\n",
    "        self.det_dataset = VOCDetection(det_root, year=year, image_set=image_set, transforms=transforms, download=download)\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.seg_dataset), len(self.det_dataset))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seg_data, seg_target = self.seg_dataset[idx]\n",
    "        det_data, det_target = self.det_dataset[idx]\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            seg_data, seg_target = self.transforms(seg_data, seg_target)\n",
    "            det_data, det_target = self.transforms(det_data, det_target)\n",
    "\n",
    "        return (seg_data, seg_target), (det_data, det_target)\n",
    "\n",
    "seg_root = \"path/to/seg_data\"\n",
    "det_root = \"path/to/det_data\"\n",
    "year = \"2012\"\n",
    "image_set = \"trainval\"\n",
    "transformations = None\n",
    "download = True\n",
    "\n",
    "combined_dataset = CombinedVOC(seg_root, det_root, year, image_set, transforms=transformations, download=download)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный код выполняет следующие действия:\n",
    "\n",
    "- Импортирует необходимые библиотеки (cv2 – для работы с изображениями, numpy – для работы с массивами данных, urllib.request – для выполнения запросов к web-ресурсам).\n",
    "\n",
    "- Загружает конфигурацию и веса для модели YOLOv3 из указанных файлов. YOLO (You Only Look Once) – это алгоритм обработки изображений, который используется для обнаружения объектов.\n",
    "\n",
    "- Загружает список классов, который может использоваться моделью YOLO для обнаружения и классификации объектов. Классы в данном случае - это типы объектов, которые YOLO может обнаружить (например, человек, автомобиль, собака и т.д.)\n",
    "\n",
    "Тем не менее, сам код не выполняет детекцию объектов, он только загружает необходимые для этой операции файлы и модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Загрузка модели и весов YOLOv3\n",
    "yolo_config = \"/Vacation/модель/yolov3.cfg\"\n",
    "yolo_weights = \"/Vacation/модель/yolov3.weights\"\n",
    "net = cv2.dnn.readNet(yolo_weights, yolo_config)\n",
    "\n",
    "# Загрузка списка классов\n",
    "class_file = \"/Vacation/data/coco.txt\"\n",
    "with open(class_file, 'r') as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По проходу через модель изображения программа будет обнаруживать и отслеживать только эти типы объектов. Это может быть полезно для приложений, которым необходимо фокусироваться только на определенных объектах и игнорировать все остальные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение выходных слоев для сети YOLO\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers_indices = net.getUnconnectedOutLayers()\n",
    "output_layers_indices = np.reshape(output_layers_indices, (output_layers_indices.size,))\n",
    "out_layers = [layer_names[i - 1] for i in output_layers_indices]\n",
    "\n",
    "# Устанавливаем классы, которые хотим обнаружить\n",
    "classes_to_look_for = [\"person\", \"cell phone\", \"laptop\", \"tv\", \"remote\"] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее прописываем код для обработки видео и выводим результат обработки видео с обнаружением телефона"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Обработка видео завершена. Обнаруженные события:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def detect_objects(frame, net, out_layers, classes_to_look_for):\n",
    "    detected_objects = []\n",
    "    height, width, channels = frame.shape\n",
    "    blob = cv2.dnn.blobFromImage(frame, scalefactor=1/255, size=(416, 416), mean=(0, 0, 0), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layer_outputs = net.forward(out_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    for output in layer_outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.4 and classes[class_id] in classes_to_look_for:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                confidences.append(float(confidence))\n",
    "                boxes.append([x, y, w, h])\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, score_threshold=0.4, nms_threshold=0.5)\n",
    "    \n",
    "    if isinstance(indices, tuple) and not indices:\n",
    "         indices = []\n",
    "    elif isinstance(indices, np.ndarray):\n",
    "        indices = indices.astype(int)\n",
    "        indices = indices.reshape(-1, 1).tolist()\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected value in 'indices': {indices}\")\n",
    "\n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        x, y, w, h = boxes[i]\n",
    "        label = str(classes[class_ids[i]])\n",
    "        detected_objects.append(label)\n",
    "        confidence = round(confidences[i], 2)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f'{label} {confidence}', (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)  \n",
    "    return frame, detected_objects\n",
    "yolo_config = \"/Vacation/модель/yolov3.cfg\"\n",
    "yolo_weights = \"/Vacation/модель/yolov3.weights\"\n",
    "net = cv2.dnn.readNet(yolo_weights, yolo_config)\n",
    "\n",
    "class_file = \"/Vacation/data/coco.txt\"\n",
    "with open(class_file, 'r') as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers_indices = net.getUnconnectedOutLayers()\n",
    "output_layers_indices = np.reshape(output_layers_indices, (output_layers_indices.size,))\n",
    "out_layers = [layer_names[i - 1] for i in output_layers_indices]\n",
    "\n",
    "classes_to_look_for = [\"person\", \"cell phone\", \"laptop\", \"tv\", \"remote\"]\n",
    "# Настройка логирования\n",
    "logging.basicConfig(filename='results.log', level=logging.INFO)\n",
    "\n",
    "def load_video():\n",
    "    return \"train_dataset_Бригады/Анализ бригад (телефон)/Есть телефон/22_11_46.mp4\"\n",
    "\n",
    "video_path = load_video()  # video_path теперь определён до его первого использования\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "\n",
    "class_file = \"/Vacation/data/coco.txt\"\n",
    "with open(class_file, 'r') as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "\n",
    "output_folder = \"/Vacation/result\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "output_video_path = os.path.join(output_folder, 'результат.mp4')\n",
    "\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, 20.0, (frame_width,frame_height))\n",
    "\n",
    "phone_in_hands = False\n",
    "phone_detection_timer = None\n",
    "phone_hold_time = 3.0\n",
    "\n",
    "events = []  # список для сохранения всех событий\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        processed_frame, detected_objects = detect_objects(frame, net, out_layers, classes_to_look_for)\n",
    "        detected_objects_count = Counter(detected_objects)\n",
    "\n",
    "        detected_phone_count = detected_objects_count.get('cell phone', 0)\n",
    "        detected_person_count = detected_objects_count.get('person', 0)\n",
    "\n",
    "        if detected_phone_count > 0:\n",
    "            if not phone_in_hands:\n",
    "                phone_detection_timer = time.time()\n",
    "                logging.info('Обнаружено событие: начало использования телефона')\n",
    "            phone_in_hands = True\n",
    "        else:\n",
    "            if phone_in_hands:\n",
    "                 logging.info('Обнаружено событие: окончание использования телефона')\n",
    "            phone_in_hands = False\n",
    "\n",
    "\n",
    "        if detected_person_count >= 2:\n",
    "            if phone_in_hands and (time.time() - phone_detection_timer) >= phone_hold_time:\n",
    "                event_msg = f\"Телефон(ы) был(и) в руках двух человек более {phone_hold_time} секунд.\"\n",
    "                events.append(event_msg)\n",
    "        elif detected_person_count == 1:\n",
    "            if phone_in_hands and (time.time() - phone_detection_timer) >= phone_hold_time:\n",
    "                event_msg = \"Телефон был в руках одного человека более 3-х секунд.\"\n",
    "                events.append(event_msg)\n",
    "       \n",
    "        cv2.imshow('Detected Objects', processed_frame)\n",
    "        out.write(processed_frame)\n",
    "\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"\\nОбработка видео завершена. Обнаруженные события:\")\n",
    "for idx, event in enumerate(events, start=1):\n",
    "    logging.info(f\"Обнаружено событие {idx}: {event}\")\n",
    "    print(f\"{idx}. {event}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из-за неправильно выбранной модели ИИ, мы можем наблюдать не совсем корректную обработку видео."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<image src=\"result/Снимок экрана 2023-09-24 в 01.31.51.png\" alt=\"Текст с описанием картинки\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<image src=\"result/Снимок экрана 2023-10-09 в 12.57.08.png\" alt=\"Текст с описанием картинки\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video controls>\n",
    "  <source src=\"/Users/kakotichi/Documents/GitHub/sf_data_science/Vacation/result/Запись экрана 2023-10-09 в 12.56.27.mov\">\n",
    "  Ваш браузер не поддерживает воспроизведение видео.\n",
    "</video>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video controls>\n",
    "  <source src=\"/Users/kakotichi/Documents/GitHub/sf_data_science/Vacation/result/Запись экрана 2023-10-09 в 13.05.39.mov\">\n",
    "  Ваш браузер не поддерживает воспроизведение видео.\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша выбранная модель Yolo3 работает вполне не плохо, но для корректной обработки видео стоило выбирать модель Yolo8, который более корректно работает, чем наша модель, т.к она обучена на большем количестве данных! Но мы видем, что в целом наша модель вполне не плохо справляется с обнаружением телефона на видео. И результат обработки видео мы можеи видеть в протоколе под названием results.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Так же мы можем посчитать среднее значение для каждого класса данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Counts: {'person': 0.0, 'cell phone': 1.394621372965322, 'laptop': 0.0, 'tv': 0.0, 'remote': 0.0}\n"
     ]
    }
   ],
   "source": [
    "average_counts = {class_name:[] for class_name in classes_to_look_for}\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "\n",
    "        processed_frame, detected_objects = detect_objects(frame, net, out_layers, classes_to_look_for)\n",
    "\n",
    "        # Подсчёт обнаруженных объектов\n",
    "        detected_objects_count = Counter(detected_objects)\n",
    "\n",
    "        # Сохранение количества для среднего значения\n",
    "        for class_name in classes_to_look_for:\n",
    "            average_counts[class_name].append(detected_objects_count.get(class_name, 0))\n",
    "\n",
    "        detected_phone_count = detected_objects_count.get('cell phone', 0)\n",
    "        detected_person_count = detected_objects_count.get('person', 0)\n",
    "\n",
    "        current_time_secs = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0 \n",
    "\n",
    "        if detected_phone_count > 0:\n",
    "            if not phone_in_hands:\n",
    "                phone_detection_timer = current_time_secs\n",
    "                logging.info('Обнаружено событие: начало использования телефона')\n",
    "            phone_in_hands = True\n",
    "        else:\n",
    "            if phone_in_hands:\n",
    "                logging.info('Обнаружено событие: окончание использования телефона')\n",
    "            phone_in_hands = False\n",
    "\n",
    "        if detected_person_count >= 2:\n",
    "            if phone_in_hands and (current_time_secs - phone_detection_timer) >= phone_hold_time:\n",
    "                event_msg = f\"Телефон(ы) был(и) в руках двух человек более {phone_hold_time} секунд.\"\n",
    "                events.append(event_msg)\n",
    "        elif detected_person_count == 1:\n",
    "            if phone_in_hands and (current_time_secs - phone_detection_timer) >= phone_hold_time:\n",
    "                event_msg = \"Телефон был в руках одного человека более 3-х секунд.\"\n",
    "                events.append(event_msg)\n",
    "\n",
    "        # Here we draw the frame and write to the video file\n",
    "        cv2.imshow('Detected Objects', processed_frame)\n",
    "        out.write(processed_frame)\n",
    "\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Calculating average counts\n",
    "average_counts = {class_name: sum(counts)/len(counts) for class_name, counts in average_counts.items()}\n",
    "print(f\"Average Counts: {average_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь мы можем наблюдать некоторую погрешность, ведь один или несколько человек всегда присуствует в кадре. А вот  определение телефона в среднем в данныых находит 1.4 мобильного телефона "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
